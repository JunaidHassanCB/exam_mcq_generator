{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Resources"
      ],
      "metadata": {
        "id": "bJ04CPCrZgDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.youtube.com/watch?v=ZzgUqFtxgXI\n",
        "* https://www.youtube.com/watch?v=7aBRk_JP-qY\n",
        "* https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/3/document-splitting"
      ],
      "metadata": {
        "id": "LkiJqtHQZiym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "m-ikwGBjMI4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Exam MCQ Generator"
      ],
      "metadata": {
        "id": "mLRXYyYMML6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Game Plan:\n",
        "\n",
        "*   Divide the document into multiple chuncks and then select chunks at random, pass them to Langchain and ask to make an MCQ out of it"
      ],
      "metadata": {
        "id": "o7UK-p8sMPZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "bmFi-Hk6Evih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv\n",
        "\n",
        "!pip install -q langchain_experimental\n",
        "!pip install -q langchain\n",
        "\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2t8uaVSEwvu",
        "outputId": "da3fc381-cc50-42c7-9779-b2c0bbd17137"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic"
      ],
      "metadata": {
        "id": "7HVRFDBtRaW_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "A1x1YGfOE_oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "pS0ZpqhCFBMR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Source"
      ],
      "metadata": {
        "id": "Uoi0Wy5cXo_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q29vd3OyXQka",
        "outputId": "70c60fb4-3c71-474e-cbd1-9005b584c331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wizard_in_oz_text_path = '/content/drive/MyDrive/datasets/llm-from-scratch/wizard_in_oz.txt'"
      ],
      "metadata": {
        "id": "Ka2XEvl8XrG1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(wizard_in_oz_text_path, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "2XeC5XxUdiEI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ET8m79disZ",
        "outputId": "38f3bd14-37a4-46c5-cb88-2692f312655b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Title: Dorothy and the Wizard in Oz\n",
            "\n",
            "\n",
            "Author: L. Frank Baum\n",
            "\n",
            "Illustrator: John R. Neill\n",
            "\n",
            "Release da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Env"
      ],
      "metadata": {
        "id": "64daLT8tFOIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_path = '/content/drive/MyDrive/credentials/data-analytics-demo/.env'"
      ],
      "metadata": {
        "id": "V3RXC1W0djkz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(env_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymW18blMFMOM",
        "outputId": "72cb29b5-32ea-4ed5-a709-9aeabff2a273"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_PALM_API_KEY = os.environ['GOOGLE_PALM_API_KEY']\n",
        "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\n",
        "OPEN_AI_API_KEY = os.environ['OPEN_AI_API_KEY']\n",
        "\n",
        "# GOOGLE_PALM_API_KEY, HUGGINGFACE_API_KEY, OPEN_AI_API_KEY"
      ],
      "metadata": {
        "id": "Er0qbi8PFNey"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "6vYSSCaaOwTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MCQModel(BaseModel):\n",
        "  question: str = Field(description=\"This is the question text\")\n",
        "  options: list[str] = Field(description=\"This is a list of multiple choices or options avalible\")\n",
        "  correct_option: str = Field(description=\"This is the correct choice or option\")"
      ],
      "metadata": {
        "id": "Qf3wdSRvDZ6q"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "pydantic_parser = PydanticOutputParser(pydantic_object=MCQModel)\n",
        "\n",
        "format_instructions = pydantic_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UomvYYs3Er4d",
        "outputId": "818b485c-71a5-4704-bce6-3600f5306893"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"question\": {\"title\": \"Question\", \"description\": \"This is the question text\", \"type\": \"string\"}, \"options\": {\"title\": \"Options\", \"description\": \"This is a list of multiple choices or options avalible\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"correct_option\": {\"title\": \"Correct Option\", \"description\": \"This is the correct choice or option\", \"type\": \"string\"}}, \"required\": [\"question\", \"options\", \"correct_option\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain_llm = ChatOpenAI(openai_api_key=OPEN_AI_API_KEY, model_name=\"gpt-3.5-turbo-0613\", verbose=False,)"
      ],
      "metadata": {
        "id": "HLQQRe4DebG9"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "felQ-ng_FZyj"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "fSgy5XPkO2Ua"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total Documents Created: {len(texts)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jRPe0bPciN",
        "outputId": "65234480-2984-48cf-8080-603e48b3237f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents Created: 287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Text:\"\n",
        "    \"\\n{text}\"\n",
        "    \"\\nGenerate an Multiple Choice Question from this text. Also return the correct option.\"\n",
        "    \"\\nDo not use any outside information\"\n",
        "    \"\\n{format_instructions}\"\n",
        ")"
      ],
      "metadata": {
        "id": "O5feLlNGRRbC"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2U1SRNNRxV3",
        "outputId": "be4f788d-8ce7-4097-bb45-4322feb44de1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['format_instructions', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'text'], template='Text:\\n{text}\\nGenerate an Multiple Choice Question from this text. Also return the correct option.\\nDo not use any outside information\\n{format_instructions}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=langchain_llm, prompt=prompt_template, output_key=\"result\")"
      ],
      "metadata": {
        "id": "q6y-XA9ySO3K"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback as get_langchain_openai_callback\n",
        "import random\n",
        "\n",
        "def get_question():\n",
        "  doc_index = random.randint(0, len(texts)-1)\n",
        "\n",
        "  with get_langchain_openai_callback() as cb_langchain:\n",
        "    response = llm_chain.invoke({\"text\": texts[doc_index], \"format_instructions\": format_instructions})\n",
        "\n",
        "  return {'doc_index': doc_index, 'mcq': response['result'], 'cb_langchain': cb_langchain}"
      ],
      "metadata": {
        "id": "tMFoHATzPiG5"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "45rsHQaGmfaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_raw = get_question()\n",
        "response_parsed = MCQModel.parse_raw(response_raw['mcq'])"
      ],
      "metadata": {
        "id": "1EbUZdLie2AV"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_parsed)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"doc_index = {response_raw['doc_index']}\")\n",
        "print('\\ncosting:\\n')\n",
        "print(f\"{response_raw['cb_langchain']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8P_jJpXe4Kd",
        "outputId": "38179e16-0394-4e56-dff4-5e91c6ccd4d0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question='Who grumbled about being an ordinary horse in a fairy country?' options=['Jim', 'Zeb', 'Dorothy', 'Ozma'] correct_option='Jim'\n",
            "\n",
            "\n",
            "doc_index = 258\n",
            "\n",
            "costing:\n",
            "\n",
            "Tokens Used: 564\n",
            "\tPrompt Tokens: 520\n",
            "\tCompletion Tokens: 44\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0008680000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_parsed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmtPT3naHFt2",
        "outputId": "1c05f6f5-fa89-4534-9dec-196935ef64b7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MCQModel(question='Who grumbled about being an ordinary horse in a fairy country?', options=['Jim', 'Zeb', 'Dorothy', 'Ozma'], correct_option='Jim')"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pydantic Demo"
      ],
      "metadata": {
        "id": "vJU1KnNYRUIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, EmailStr, validator\n",
        "\n",
        "class User(BaseModel):\n",
        "  name: str\n",
        "  # email: EmailStr\n",
        "  email: str\n",
        "  account_id: int\n",
        "\n",
        "  @validator(\"account_id\")\n",
        "  def validate_account_id(cls, value):\n",
        "    if value<=0:\n",
        "      raise ValueError(f'Account ID cannot be negative: {value}')\n",
        "\n",
        "    return value\n",
        "\n",
        "user = User(name='jack', email='a@b.com', account_id=1234)\n",
        "\n",
        "print(user)\n",
        "print(user.json())\n",
        "print(user.dict())\n",
        "print(user.parse_raw(user.json()))"
      ],
      "metadata": {
        "id": "KHFMUsI0fRll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187479f6-451a-477f-ded6-62c236f1f691"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='jack' email='a@b.com' account_id=1234\n",
            "{\"name\": \"jack\", \"email\": \"a@b.com\", \"account_id\": 1234}\n",
            "{'name': 'jack', 'email': 'a@b.com', 'account_id': 1234}\n",
            "name='jack' email='a@b.com' account_id=1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkYbAdbrR0gV"
      },
      "execution_count": 90,
      "outputs": []
    }
  ]
}