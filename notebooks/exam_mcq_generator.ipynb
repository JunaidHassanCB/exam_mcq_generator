{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Resources"
      ],
      "metadata": {
        "id": "bJ04CPCrZgDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://www.youtube.com/watch?v=ZzgUqFtxgXI\n",
        "* https://www.youtube.com/watch?v=7aBRk_JP-qY\n",
        "* https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/3/document-splitting"
      ],
      "metadata": {
        "id": "LkiJqtHQZiym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "m-ikwGBjMI4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Exam MCQ Generator"
      ],
      "metadata": {
        "id": "mLRXYyYMML6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Game Plan:\n",
        "\n",
        "*   Divide the document into multiple chuncks and then select chunks at random, pass them to Langchain and ask to make an MCQ out of it"
      ],
      "metadata": {
        "id": "o7UK-p8sMPZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installs"
      ],
      "metadata": {
        "id": "bmFi-Hk6Evih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv\n",
        "\n",
        "!pip install -q langchain_experimental\n",
        "!pip install -q langchain\n",
        "\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2t8uaVSEwvu",
        "outputId": "da3fc381-cc50-42c7-9779-b2c0bbd17137"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pydantic"
      ],
      "metadata": {
        "id": "7HVRFDBtRaW_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "A1x1YGfOE_oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "pS0ZpqhCFBMR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Source"
      ],
      "metadata": {
        "id": "Uoi0Wy5cXo_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q29vd3OyXQka",
        "outputId": "70c60fb4-3c71-474e-cbd1-9005b584c331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wizard_in_oz_text_path = '/content/drive/MyDrive/datasets/llm-from-scratch/wizard_in_oz.txt'"
      ],
      "metadata": {
        "id": "Ka2XEvl8XrG1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(wizard_in_oz_text_path, 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "2XeC5XxUdiEI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ET8m79disZ",
        "outputId": "38f3bd14-37a4-46c5-cb88-2692f312655b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Title: Dorothy and the Wizard in Oz\n",
            "\n",
            "\n",
            "Author: L. Frank Baum\n",
            "\n",
            "Illustrator: John R. Neill\n",
            "\n",
            "Release da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Env"
      ],
      "metadata": {
        "id": "64daLT8tFOIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_path = '/content/drive/MyDrive/credentials/data-analytics-demo/.env'"
      ],
      "metadata": {
        "id": "V3RXC1W0djkz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(env_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymW18blMFMOM",
        "outputId": "72cb29b5-32ea-4ed5-a709-9aeabff2a273"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_PALM_API_KEY = os.environ['GOOGLE_PALM_API_KEY']\n",
        "HUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\n",
        "OPEN_AI_API_KEY = os.environ['OPEN_AI_API_KEY']\n",
        "\n",
        "# GOOGLE_PALM_API_KEY, HUGGINGFACE_API_KEY, OPEN_AI_API_KEY"
      ],
      "metadata": {
        "id": "Er0qbi8PFNey"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "6vYSSCaaOwTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "langchain_llm = ChatOpenAI(openai_api_key=OPEN_AI_API_KEY, model_name=\"gpt-3.5-turbo-0613\", verbose=False,)"
      ],
      "metadata": {
        "id": "HLQQRe4DebG9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "felQ-ng_FZyj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "fSgy5XPkO2Ua"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total Documents Created: {len(texts)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jRPe0bPciN",
        "outputId": "9eb11590-327f-4594-b768-502abe8cf69e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents Created: 287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Text:\"\n",
        "    \"\\n{text}\"\n",
        "    \"\\nGenerate an Multiple Choice Question from this text\"\n",
        "    \"\\nDo not use any outside information\"\n",
        ")"
      ],
      "metadata": {
        "id": "O5feLlNGRRbC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2U1SRNNRxV3",
        "outputId": "884f873b-daf8-4aa2-9099-d11ba8e337f4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='Text:\\n{text}\\nGenerate an Multiple Choice Question from this text\\nDo not use any outside information'))])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=langchain_llm,prompt=prompt_template,output_key=\"result\")"
      ],
      "metadata": {
        "id": "q6y-XA9ySO3K"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback as get_langchain_openai_callback\n",
        "import random\n",
        "\n",
        "def get_question():\n",
        "  doc_index = random.randint(0, len(texts)-1)\n",
        "\n",
        "  with get_langchain_openai_callback() as cb_langchain:\n",
        "    response = llm_chain.invoke({\"text\": texts[doc_index]})\n",
        "\n",
        "  return {'doc_index': doc_index, 'mcq': response['result'], 'cb_langchain': cb_langchain}"
      ],
      "metadata": {
        "id": "tMFoHATzPiG5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "45rsHQaGmfaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_question()"
      ],
      "metadata": {
        "id": "1EbUZdLie2AV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['mcq'])\n",
        "\n",
        "print('\\n')\n",
        "print(f\"doc_index = {response['doc_index']}\")\n",
        "print('\\ncosting:\\n')\n",
        "print(f\"{response['cb_langchain']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8P_jJpXe4Kd",
        "outputId": "441cad54-6735-4030-cb2f-b8480407cc62"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What happened to the ground while Dorothy and Zeb were in the buggy?\n",
            "\n",
            "a) It opened in a wide crack and then closed again\n",
            "b) It started to sway dangerously from side to side\n",
            "c) It rose up before them\n",
            "d) It made a sharp crash and a roar\n",
            "\n",
            "\n",
            "doc_index = 14\n",
            "\n",
            "costing:\n",
            "\n",
            "Tokens Used: 336\n",
            "\tPrompt Tokens: 278\n",
            "\tCompletion Tokens: 58\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.000533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pydantic Demo"
      ],
      "metadata": {
        "id": "vJU1KnNYRUIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, EmailStr, validator\n",
        "\n",
        "class User(BaseModel):\n",
        "  name: str\n",
        "  # email: EmailStr\n",
        "  email: str\n",
        "  account_id: int\n",
        "\n",
        "  @validator(\"account_id\")\n",
        "  def validate_account_id(cls, value):\n",
        "    if value<=0:\n",
        "      raise ValueError(f'Account ID cannot be negative: {value}')\n",
        "\n",
        "    return value\n",
        "\n",
        "user = User(name='jack', email='a@b.com', account_id=1234)\n",
        "\n",
        "print(user)\n",
        "print(user.json())\n",
        "print(user.dict())\n",
        "print(user.parse_raw(user.json()))"
      ],
      "metadata": {
        "id": "KHFMUsI0fRll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3ba423-06f1-4e93-cd92-5908e72039ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='jack' email='a@b.com' account_id=1234\n",
            "{\"name\": \"jack\", \"email\": \"a@b.com\", \"account_id\": 1234}\n",
            "{'name': 'jack', 'email': 'a@b.com', 'account_id': 1234}\n",
            "name='jack' email='a@b.com' account_id=1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkYbAdbrR0gV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}